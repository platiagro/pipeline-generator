# This workflow will install Python dependencies, run tests and lint with a single version of Python
# For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions

name: Python application

on:
  push:
    branches: [ master ]
  pull_request:
    branches: [ master ]

jobs:
  build:

    runs-on: ubuntu-latest

    steps:
    - name: Set Docker TAG
      id: var
      run: |
        echo ${{ github.ref }}
        if [ ${{ github.ref }} = "refs/heads/master" ]; then
          echo ::set-output name=TAG::0.1.0
        else
          echo ::set-output name=TAG::0.1.0-SNAPSHOT
        fi
    - uses: actions/checkout@v2
    - name: Set up Python 3.6
      uses: actions/setup-python@v1
      with:
        python-version: 3.6
    - name: Install Kubernetes 1.15.7
      run: |
        curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add
        sudo swapoff -a
        sudo apt-add-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"
        sudo apt-get install -y --allow-downgrades kubeadm=1.15.7-00 kubelet=1.15.7-00 kubectl=1.15.7-00
        sudo kubeadm init
        sudo mkdir -p $HOME/.kube
        sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
        sudo chown $(id -u):$(id -g) $HOME/.kube/config
        kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
        kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=admin --user=kubelet --group=system:serviceaccounts
        kubectl taint nodes --all node-role.kubernetes.io/master-
        while [ $(kubectl get node |tail -n 1|awk '{print $2}') != "Ready" ]; do echo waiting nodes to be ready...; sleep 1; done
    - name: Create Persistent Volumes
      run: |
        for i in `seq 1 10`; do
          sudo mkdir -p "/mnt/disks/vol-$i"
          sudo mount -t tmpfs -o size=20G "vol-$i" "/mnt/disks/vol-$i"
        done

        cat <<EOF | kubectl apply -f -
          kind: StorageClass
          apiVersion: storage.k8s.io/v1
          metadata:
            name: local-storage
          provisioner: kubernetes.io/no-provisioner
          reclaimPolicy: Delete
        EOF

        kubectl patch storageclass local-storage -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

        cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: local-provisioner-config
            namespace: default
            labels:
              heritage: "Tiller"
              release: "release-name"
              chart: provisioner-2.3.2
          data:
            storageClassMap: |
              local-storage:
                hostDir: /mnt/disks
                mountDir: /mnt/disks
                blockCleanerCommand:
                  - "/scripts/shred.sh"
                  - "2"
                volumeMode: Filesystem
        EOF

        cat <<EOF | kubectl apply -f -
          apiVersion: apps/v1
          kind: DaemonSet
          metadata:
            name: local-volume-provisioner
            namespace: default
            labels:
              app: local-volume-provisioner
              heritage: "Tiller"
              release: "release-name"
              chart: provisioner-2.3.2
          spec:
            selector:
              matchLabels:
                app: local-volume-provisioner
            template:
              metadata:
                labels:
                  app: local-volume-provisioner
              spec:
                serviceAccountName: local-storage-admin
                containers:
                  - image: "quay.io/external_storage/local-volume-provisioner:v2.3.2"
                    name: provisioner
                    securityContext:
                      privileged: true
                    env:
                    - name: MY_NODE_NAME
                      valueFrom:
                        fieldRef:
                          fieldPath: spec.nodeName
                    - name: MY_NAMESPACE
                      valueFrom:
                        fieldRef:
                          fieldPath: metadata.namespace
                    - name: JOB_CONTAINER_IMAGE
                      value: "quay.io/external_storage/local-volume-provisioner:v2.3.2"
                    volumeMounts:
                      - mountPath: /etc/provisioner/config
                        name: provisioner-config
                        readOnly: true
                      - mountPath: /dev
                        name: provisioner-dev
                      - mountPath: /mnt/disks
                        name: disks
                        mountPropagation: "HostToContainer"
                volumes:
                  - name: provisioner-config
                    configMap:
                      name: local-provisioner-config
                  - name: provisioner-dev
                    hostPath:
                      path: /dev
                  - name: disks
                    hostPath:
                      path: /mnt/disks
        EOF

        cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ServiceAccount
          metadata:
            name: local-storage-admin
            namespace: default
            labels:
              heritage: "Tiller"
              release: "release-name"
              chart: provisioner-2.3.2
        EOF

        cat <<EOF | kubectl apply -f -
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRoleBinding
          metadata:
            name: local-storage-provisioner-pv-binding
            labels:
              heritage: "Tiller"
              release: "release-name"
              chart: provisioner-2.3.2
          subjects:
          - kind: ServiceAccount
            name: local-storage-admin
            namespace: default
          roleRef:
            kind: ClusterRole
            name: system:persistent-volume-provisioner
            apiGroup: rbac.authorization.k8s.io
        EOF

        cat <<EOF | kubectl apply -f -
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRole
          metadata:
            name: local-storage-provisioner-node-clusterrole
            labels:
              heritage: "Tiller"
              release: "release-name"
              chart: provisioner-2.3.2
          rules:
          - apiGroups: [""]
            resources: ["nodes"]
            verbs: ["get"]
        EOF

        cat <<EOF | kubectl apply -f -
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRoleBinding
          metadata:
            name: local-storage-provisioner-node-binding
            labels:
              heritage: "Tiller"
              release: "release-name"
              chart: provisioner-2.3.2
          subjects:
          - kind: ServiceAccount
            name: local-storage-admin
            namespace: default
          roleRef:
            kind: ClusterRole
            name: local-storage-provisioner-node-clusterrole
            apiGroup: rbac.authorization.k8s.io
        EOF

        sleep 10
        kubectl wait --for=condition=Ready pods --all
    - name: Install Kubeflow Pipelines
      run: |
        kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION"
        kubectl wait --for condition=established --timeout=60s crd/applications.app.k8s.io
        kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic?ref=$PIPELINE_VERSION"
        kubectl -n kubeflow wait applications/pipeline --for condition=Ready --timeout=1800s
        nohup kubectl -n kubeflow port-forward svc/ml-pipeline-ui 8080:80 &
        sleep 10
        curl -k -L localhost:8080/pipeline/apis/v1beta1/experiments?
      env:
        PIPELINE_VERSION: 0.5.1
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements/requirements.txt
        pip install .
    - name: Test with pytest
      run: |
        curl -k -L localhost:8080/pipeline/apis/v1beta1/experiments?
        pip install pytest pytest-cov
        coverage erase
        coverage run --branch --source=pipelines -m pytest
        coverage xml -i
      env:
        KF_PIPELINES_ENDPOINT: 127.0.0.1:8080/pipeline
    - name: SonarCloud Scan
      uses: sonarsource/sonarcloud-github-action@master
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
    - name: Build and push image
      uses: docker/build-push-action@v1
      with:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}
        registry: registry.hub.docker.com
        repository: platiagro/pipelines
        tags: ${{ steps.var.outputs.TAG }}
